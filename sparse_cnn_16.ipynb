{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]:\n",
    "# Import required modules\n",
    "import gc\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import h5py\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, LSTM, Input, AlphaDropout, Activation, Reshape, Input\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.models as Model\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import GlorotUniform, HeNormal\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pickle\n",
    "import sys\n",
    "import operator\n",
    "from numpy import linalg as la\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acede456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "# data pre-processing\n",
    "with open(\"./data/RML2016.10b.dat\", \"rb\") as p:\n",
    "    Xd = pickle.load(p, encoding='latin1')\n",
    "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
    "print(\"length of snr\",len(snrs))\n",
    "print(\"length of mods\",len(mods))\n",
    "X = [] \n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
    "X = np.vstack(X)\n",
    "print(\"shape of X\", np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a75c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "# Partition the dataset into training and testing datasets\n",
    "np.random.seed(2016)     # Random seed value for the partitioning (Also used for random subsampling)\n",
    "n_examples = X.shape[0]\n",
    "n_train = n_examples // 2\n",
    "train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)\n",
    "test_idx = list(set(range(0,n_examples))-set(train_idx))\n",
    "X_train = X[train_idx]\n",
    "X_test =  X[test_idx]\n",
    "def to_onehot(yy):\n",
    "    yy1 = np.zeros([len(yy), max(yy)+1])\n",
    "    yy1[np.arange(len(yy)),yy] = 1\n",
    "    return yy1\n",
    "Y_train = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))\n",
    "Y_test = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "print('training started')\n",
    "in_shp = list(X_train.shape[1:])\n",
    "print(X_train.shape, in_shp)\n",
    "classes = mods\n",
    "\n",
    "# Resnet Architecture\n",
    "def residual_stack(x):\n",
    "    def residual_unit(y,_strides=1):\n",
    "        shortcut_unit=y\n",
    "        # 1x1 conv linear\n",
    "        y = layers.Conv1D(32, kernel_size=5,data_format='channels_first',strides=_strides,padding='same',activation='relu')(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.Conv1D(32, kernel_size=5,data_format='channels_first',strides=_strides,padding='same',activation='linear')(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        # add batch normalization\n",
    "        y = layers.add([shortcut_unit,y])\n",
    "        return y\n",
    "\n",
    "    x = layers.Conv1D(32, data_format='channels_first',kernel_size=1, padding='same',activation='linear')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = residual_unit(x)\n",
    "    x = residual_unit(x)\n",
    "    # maxpool for down sampling\n",
    "    x = layers.MaxPooling1D(data_format='channels_first')(x)\n",
    "    return x\n",
    "\n",
    "inputs=layers.Input(shape=in_shp)\n",
    "x = residual_stack(inputs)  # output shape (32,64)\n",
    "x = residual_stack(x)    # out shape (32,32)\n",
    "x = residual_stack(x)    # out shape (32,16)    # Comment this when the input dimensions are 1/32 or lower\n",
    "x = residual_stack(x)    # out shape (32,8)     # Comment this when the input dimensions are 1/16 or lower\n",
    "x = residual_stack(x)    # out shape (32,4)     # Comment this when the input dimensions are 1/8 or lower\n",
    "x = Flatten()(x)\n",
    "x = Dense(128,kernel_initializer=\"he_normal\", activation=\"selu\", name=\"dense1\")(x)\n",
    "x = AlphaDropout(0.1)(x)\n",
    "x = Dense(128,kernel_initializer=\"he_normal\", activation=\"selu\", name=\"dense2\")(x)\n",
    "x = AlphaDropout(0.1)(x)\n",
    "x = Dense(len(classes),kernel_initializer=\"he_normal\", activation=\"softmax\", name=\"dense3\")(x)\n",
    "x_out = Reshape([len(classes)])(x)\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=x_out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "# Set up some params \n",
    "nb_epoch = 500     # number of epochs to train on\n",
    "batch_size = 1024  # training batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c62219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "# Train the Model\n",
    "# perform training ...\n",
    "#   - call the main training loop in keras for our network+dataset\n",
    "filepath = './models/nopca/resnet_10b_nopca_wts.h5'\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(X_train,\n",
    "    Y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=nb_epoch,\n",
    "    verbose=2,\n",
    "    validation_split=0.25,\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='auto')\n",
    "    ])\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "# we re-load the best weights once training is finished\n",
    "model.load_weights(filepath)\n",
    "model.save('./models/nopca/resnet_10b_nopca.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbb363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d319511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "# Plot confusion matrix\n",
    "acc = {}\n",
    "for snr in snrs:\n",
    "    # extract classes @ SNR\n",
    "    test_SNRs = list(map(lambda x: lbl[x][1], test_idx))\n",
    "    test_X_i = X_test[np.where(np.array(test_SNRs)==snr)]\n",
    "    test_Y_i = Y_test[np.where(np.array(test_SNRs)==snr)]    \n",
    "    print(test_X_i.shape[0])\n",
    "\n",
    "    # estimate classes\n",
    "    test_Y_i_hat = model.predict(test_X_i)\n",
    "    conf = np.zeros([len(classes),len(classes)])\n",
    "    confnorm = np.zeros([len(classes),len(classes)])\n",
    "    for i in range(0,test_X_i.shape[0]):\n",
    "        j = list(test_Y_i[i,:]).index(1)\n",
    "        k = int(np.argmax(test_Y_i_hat[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(classes)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "    #plt.figure()\n",
    "    #plot_confusion_matrix(confnorm, labels=classes, title=\"ConvNet Confusion Matrix (SNR=%d)\"%(snr))\n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print(\"Overall Accuracy for SNR = \" + str(snr) + \": \", cor / (cor+ncor))\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
